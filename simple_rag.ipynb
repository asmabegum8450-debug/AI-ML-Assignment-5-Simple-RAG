{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbd04fd",
   "metadata": {},
   "source": [
    "# Simple Retrieval-Augmented Generation (RAG) System\n",
    "\n",
    "**Course Assignment: Implementing a Simple Retrieval-Augmented Generation (RAG) System**  \n",
    "**Student:** Asma Begum \n",
    "\n",
    "This notebook implements a basic RAG pipeline with the following steps:\n",
    "\n",
    "1. **Knowledge Base (KB) Creation & Chunking**\n",
    "2. **Embedding & Indexing** using a Sentence Transformer\n",
    "3. **Retrieval** via cosine similarity\n",
    "4. **Generation & Prompt Augmentation** using a small LLM (Flan-T5)\n",
    "5. **Testing** with three test cases:\n",
    "   - Test Case 1: Factual (answer in KB)\n",
    "   - Test Case 2: Foil/General (not in KB)\n",
    "   - Test Case 3: Synthesis (requires combining multiple chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a4cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in a fresh environment, uncomment this cell to install dependencies.\n",
    "# !pip install -q sentence-transformers transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55cd6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from typing import List, Dict, Any, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae0e17",
   "metadata": {},
   "source": [
    "## 1. Knowledge Base (KB) Creation & Chunking\n",
    "\n",
    "We create a small, **fictional** knowledge base about an orbital biodome project called *AstraGarden*, \n",
    "which is not part of any real-world dataset. This satisfies the assignment requirement that the KB \n",
    "covers a narrow, custom topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720884e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_text = \"\"\"\n",
    "AstraGarden is a small experimental orbital biodome that orbits 500 kilometers above Earth. \n",
    "The biodome was launched in 2037 as part of an international collaboration to test long-term \n",
    "food production in microgravity. AstraGarden contains three main habitat rings: the Crop Ring, \n",
    "the Research Ring, and the Habitat Ring. The Crop Ring uses rotating sections to simulate \n",
    "partial gravity for soil-based crops like potatoes, dwarf wheat, and leafy greens.\n",
    "\n",
    "The Research Ring is dedicated to studying plant biology, closed-loop life support systems, \n",
    "and microbial ecosystems. Scientists on AstraGarden run experiments on how root systems \n",
    "develop in low gravity, and how different lighting schedules affect plant growth and nutrient content. \n",
    "Waste from the crew and unused plant matter is recycled through bio-reactors that convert organic matter \n",
    "into fertilizer and usable water, making the station mostly self-sustaining.\n",
    "\n",
    "The Habitat Ring supports up to twelve crew members at a time and includes living quarters, \n",
    "a small medical bay, exercise facilities, and a panoramic observation deck. Crew members rotate in \n",
    "six-month missions, with a strong focus on mental health and social routines to reduce isolation. \n",
    "AstraGarden's mission is to demonstrate that a small, modular biodome can provide reliable food, \n",
    "clean air, and water for future deep space missions to Mars and beyond.\n",
    "\"\"\"\n",
    "\n",
    "# Simple chunking: split by double newlines (paragraphs)\n",
    "raw_chunks = [chunk.strip() for chunk in kb_text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "for i, c in enumerate(raw_chunks):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(c, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d369d89",
   "metadata": {},
   "source": [
    "## 2. Embedding & Indexing\n",
    "\n",
    "We use a pre-trained Sentence Transformer (`sentence-transformers/all-MiniLM-L6-v2`) \n",
    "to create embeddings for each KB chunk.\n",
    "\n",
    "We then store:\n",
    "- The text of each chunk\n",
    "- Its vector embedding\n",
    "\n",
    "This acts as a simple in-memory **vector store**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b634ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer for embeddings\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "# Compute embeddings for chunks\n",
    "kb_embeddings = embedder.encode(raw_chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Simple in-memory \"index\"\n",
    "kb_index = {\n",
    "    \"chunks\": raw_chunks,\n",
    "    \"embeddings\": kb_embeddings,\n",
    "}\n",
    "\n",
    "kb_index[\"embeddings\"].shape, len(kb_index[\"chunks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689b099",
   "metadata": {},
   "source": [
    "## 3. Retrieval: Similarity Search\n",
    "\n",
    "We implement a function:\n",
    "\n",
    "```python\n",
    "retrieve_relevant_chunks(query, k=2)\n",
    "```\n",
    "\n",
    "which:\n",
    "1. Embeds the query using the same Sentence Transformer.\n",
    "2. Computes cosine similarity between the query vector and each KB chunk embedding.\n",
    "3. Returns the top-`k` most similar chunks and their scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10))\n",
    "\n",
    "def retrieve_relevant_chunks(\n",
    "    query: str,\n",
    "    k: int = 2\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"\n",
    "    Returns top-k (chunk_text, similarity_score, chunk_index) tuples.\n",
    "    \"\"\"\n",
    "    query_vec = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    \n",
    "    sims = []\n",
    "    for idx, chunk_vec in enumerate(kb_index[\"embeddings\"]):\n",
    "        sim = cosine_similarity(query_vec, chunk_vec)\n",
    "        sims.append((idx, sim))\n",
    "    \n",
    "    # Sort by similarity descending\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for idx, sim in sims[:k]:\n",
    "        results.append((kb_index[\"chunks\"][idx], sim, idx))\n",
    "    return results\n",
    "\n",
    "# Quick retrieval sanity check\n",
    "test_query = \"How many people can live in AstraGarden at once?\"\n",
    "retrieved = retrieve_relevant_chunks(test_query, k=2)\n",
    "for text, score, idx in retrieved:\n",
    "    print(f\"Chunk index: {idx}, similarity: {score:.4f}\")\n",
    "    print(text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b758a",
   "metadata": {},
   "source": [
    "## 4. Generation & Prompt Augmentation\n",
    "\n",
    "We use a small seq2seq LLM from Hugging Face: **`google/flan-t5-small`**.\n",
    "\n",
    "The generation step:\n",
    "1. Takes the **original user query**.\n",
    "2. Appends the **retrieved context chunks**.\n",
    "3. Asks the model to answer **only using the context**, and to say \n",
    "   `\"I don't know based on the provided context.\"` if the answer cannot be found.\n",
    "\n",
    "This helps mitigate hallucination by explicitly grounding the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "\n",
    "rag_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "def build_rag_prompt(query: str, context_chunks: List[str]) -> str:\n",
    "    context_str = \"\\n\\n\".join([f\"- {c}\" for c in context_chunks])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an assistant that answers questions based ONLY on the provided context about AstraGarden.\n",
    "If the answer is not clearly in the context, respond with: \"I don't know based on the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def rag_answer(query: str, k: int = 2, max_new_tokens: int = 128) -> Dict[str, Any]:\n",
    "    retrieved = retrieve_relevant_chunks(query, k=k)\n",
    "    retrieved_chunks = [r[0] for r in retrieved]\n",
    "    retrieved_indices = [r[2] for r in retrieved]\n",
    "    retrieved_scores = [r[1] for r in retrieved]\n",
    "    \n",
    "    prompt = build_rag_prompt(query, retrieved_chunks)\n",
    "    \n",
    "    generation = rag_pipeline(prompt, max_new_tokens=max_new_tokens)[0][\"generated_text\"]\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_chunks\": retrieved_chunks,\n",
    "        \"retrieved_indices\": retrieved_indices,\n",
    "        \"retrieved_scores\": retrieved_scores,\n",
    "        \"prompt\": prompt,\n",
    "        \"answer\": generation,\n",
    "    }\n",
    "\n",
    "# Quick test\n",
    "demo = rag_answer(\"What is the mission of AstraGarden?\", k=2)\n",
    "demo[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3550bbc",
   "metadata": {},
   "source": [
    "## 5. Testing: Three Distinct Test Cases\n",
    "\n",
    "We run three test cases:\n",
    "\n",
    "1. **Factual Case (in KB)**  \n",
    "   Question directly answerable from the KB.\n",
    "\n",
    "2. **Foil / General Knowledge Case (not in KB)**  \n",
    "   Question not covered by the KB to test if the model admits uncertainty.\n",
    "\n",
    "3. **Synthesis Case (multi-chunk reasoning)**  \n",
    "   Question requiring combining information from multiple chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0043eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = {\n",
    "    \"factual\": \"What kinds of crops are grown in AstraGarden?\",\n",
    "    \"foil\": \"Who is the current director of NASA?\",\n",
    "    \"synthesis\": \"How does AstraGarden recycle waste to support long-term missions?\",\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, query in test_cases.items():\n",
    "    print(f\"=== Test Case: {name.upper()} ===\")\n",
    "    res = rag_answer(query, k=2)\n",
    "    results[name] = res\n",
    "    \n",
    "    print(\"Query:\", res[\"query\"], \"\\n\")\n",
    "    print(\"Retrieved chunk indices:\", res[\"retrieved_indices\"])\n",
    "    print(\"Similarity scores:\", [f\"{s:.4f}\" for s in res[\"retrieved_scores\"]], \"\\n\")\n",
    "    \n",
    "    print(\"Retrieved Chunks:\")\n",
    "    for i, chunk in enumerate(res[\"retrieved_chunks\"]):\n",
    "        print(f\"[Chunk {res['retrieved_indices'][i]}]\")\n",
    "        print(chunk, \"\\n\")\n",
    "    \n",
    "    print(\"Final Answer:\")\n",
    "    print(res[\"answer\"])\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  }

